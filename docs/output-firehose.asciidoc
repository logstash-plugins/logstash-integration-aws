:integration: aws
:plugin: firehose
:type: output
:default_codec: json

///////////////////////////////////////////
START - GENERATED VARIABLES, DO NOT EDIT!
///////////////////////////////////////////
:version: %VERSION%
:release_date: %RELEASE_DATE%
:changelog_url: %CHANGELOG_URL%
:include_path: ../../../../logstash/docs/include
///////////////////////////////////////////
END - GENERATED VARIABLES, DO NOT EDIT!
///////////////////////////////////////////

[id="plugins-{type}s-{plugin}"]
=== Firehose output plugin

include::{include_path}/plugin_header-integration.asciidoc[]

==== Description

Push events to an Amazon Web Services (AWS) Data Firehose.

Amazon Data Firehose is a fully managed service for delivering real-time streaming data to destinations such as Amazon services or HTTP endpoints owned by supported third-party service providers.
See : https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html

This plugin use the AWS SDK to send data to the Firehose stream.
See https://docs.aws.amazon.com/firehose/latest/dev/basic-write.html#writing-with-sdk

Your identity must have the following permissions on the stream:
* `firehose:PutRecordBatch`

==== Batch Publishing

This output publishes messages to Firehose in batches in order to optimize event throughput and increase performance.
This is done using the `PutRecordBatch` API.
See https://docs.aws.amazon.com/firehose/latest/APIReference/API_PutRecordBatch.html

When publishing messages to Firehose in batches, the following service limits must be respected :
* Each PutRecordBatch request supports up to 500 records.
* Each record in the request can be as large as 1,000 KB.
* All records in the request can be as large as 4 MB.

This plugin will dynamically adjust the size of the batch published to Firehose in order to ensure that the total payload size does not exceed the limits.

[id="plugins-{type}s-{plugin}-options"]

==== Firehose Output Configuration Options

This plugin supports the following configuration options plus the <<plugins-{type}s-{plugin}-common-options>> described later.

[cols="<,<,<",options="header",]
|=======================================================================
|Setting |Input type|Required
| <<plugins-{type}s-{plugin}-access_key_id>> |<<string,string>>|No
| <<plugins-{type}s-{plugin}-aws_credentials_file>> |<<string,string>>|No
| <<plugins-{type}s-{plugin}-batch_max_count>> |<<number,number>>|No
| <<plugins-{type}s-{plugin}-delivery_stream_name>> |<<string,string>>|No
| <<plugins-{type}s-{plugin}-endpoint>> |<<string,string>>|No
| <<plugins-{type}s-{plugin}-proxy_uri>> |<<string,string>>|No
| <<plugins-{type}s-{plugin}-region>> |<<string,string>>|No
| <<plugins-{type}s-{plugin}-record_max_size_bytes>> |<<bytes,bytes>>|No
| <<plugins-{type}s-{plugin}-record_total_max_size_bytes>> |<<bytes,bytes>>|No
| <<plugins-{type}s-{plugin}-role_arn>> |<<string,string>>|No
| <<plugins-{type}s-{plugin}-role_session_name>> |<<string,string>>|No
| <<plugins-{type}s-{plugin}-secret_access_key>> |<<string,string>>|No
| <<plugins-{type}s-{plugin}-session_token>> |<<string,string>>|No
| <<plugins-{type}s-{plugin}-use_aws_bundled_ca>> |<<boolean,boolean>>|No
|=======================================================================

Also see <<plugins-{type}s-{plugin}-common-options>> for a list of options supported by all output plugins.

&nbsp;

[id="plugins-{type}s-{plugin}-access_key_id"]
===== `access_key_id` 

  * Value type is <<string,string>>
  * There is no default value for this setting.

This plugin uses the AWS SDK and supports several ways to get credentials, which will be tried in this order:

1. Static configuration, using `access_key_id` and `secret_access_key` params in logstash plugin config
2. External credentials file specified by `aws_credentials_file`
3. Environment variables `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY`
4. Environment variables `AMAZON_ACCESS_KEY_ID` and `AMAZON_SECRET_ACCESS_KEY`
5. IAM Instance Profile (available when running inside EC2)

[id="plugins-{type}s-{plugin}-aws_credentials_file"]
===== `aws_credentials_file` 

  * Value type is <<string,string>>
  * There is no default value for this setting.

Path to YAML file containing a hash of AWS credentials.
This file will only be loaded if `access_key_id` and
`secret_access_key` aren't set.
The contents of the file should look like this:

[source,ruby]
----------------------------------
    :access_key_id: "12345"
    :secret_access_key: "54321"
----------------------------------

[id="plugins-{type}s-{plugin}-batch_max_count"]
===== `batch_max_count`

* Value type is <<number,number>>
* Default value is `500`

The maximum number of records to be sent in each batch.

[id="plugins-{type}s-{plugin}-delivery_stream_name"]
===== `delivery_stream_name`

* Value type is <<string,string>>

The name of the delivery stream.
Note that this is just the name of the stream, not the URL or ARN.

[id="plugins-{type}s-{plugin}-endpoint"]
===== `endpoint`

  * Value type is <<string,string>>
  * There is no default value for this setting.

The endpoint to connect to.
By default it is constructed using the value of `region`.
This is useful when connecting to S3 compatible services, but beware that these aren't guaranteed to work correctly with the AWS SDK.

[id="plugins-{type}s-{plugin}-record_max_size_bytes"]
===== `record_max_size_bytes`

  * Value type is <<bytes,bytes>>
  * There is `1_000_000`.

The maximum number of bytes for any record sent to Firehose.
Messages exceeding this size will be dropped.
See https://docs.aws.amazon.com/firehose/latest/APIReference/API_PutRecordBatch.html

[id="plugins-{type}s-{plugin}-record_total_max_size_bytes"]
===== `record_total_max_size_bytes`

  * Value type is <<bytes,bytes>>
  * There is `4_000_000`.

The maximum number of bytes for all records sent to Firehose.
See https://docs.aws.amazon.com/firehose/latest/APIReference/API_PutRecordBatch.html

[id="plugins-{type}s-{plugin}-proxy_uri"]
===== `proxy_uri`

  * Value type is <<string,string>>
  * There is no default value for this setting.

URI to proxy server if required

[id="plugins-{type}s-{plugin}-region"]
===== `region` 

  * Value type is <<string,string>>
  * Default value is `"us-east-1"`

The AWS Region

[id="plugins-{type}s-{plugin}-role_arn"]
===== `role_arn`

  * Value type is <<string,string>>
  * There is no default value for this setting.

The AWS IAM Role to assume, if any.
This is used to generate temporary credentials, typically for cross-account access.
See the https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRole.html[AssumeRole API documentation] for more information.

[id="plugins-{type}s-{plugin}-role_session_name"]
===== `role_session_name`

  * Value type is <<string,string>>
  * Default value is `"logstash"`

Session name to use when assuming an IAM role.

[id="plugins-{type}s-{plugin}-secret_access_key"]
===== `secret_access_key` 

  * Value type is <<string,string>>
  * There is no default value for this setting.

The AWS Secret Access Key

[id="plugins-{type}s-{plugin}-session_token"]
===== `session_token` 

  * Value type is <<string,string>>
  * There is no default value for this setting.

The AWS Session token for temporary credential

[id="plugins-{type}s-{plugin}-use_aws_bundled_ca"]
===== `use_aws_bundled_ca`

* Value type is <<boolean,boolean>>
* Default value is `false`

Use bundled CA certificates that ship with AWS SDK to verify SSL peer certificates.
For cases where the default certificates are unavailable, e.g. Windows, you can set this to `true`.

[id="plugins-{type}s-{plugin}-common-options"]
include::{include_path}/{type}.asciidoc[]

:default_codec!:
